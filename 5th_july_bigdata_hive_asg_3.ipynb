{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea377304",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdb497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "def read_hadoop_config(config_file_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file_path)\n",
    "    \n",
    "    core_components = config.sections()\n",
    "    \n",
    "    return core_components\n",
    "\n",
    "hadoop_config_file = 'hadoop-config-file.conf'\n",
    "core_components = read_hadoop_config(hadoop_config_file)\n",
    "\n",
    "print(\"Core Components of Hadoop:\")\n",
    "for component in core_components:\n",
    "    print(component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91001c16",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edae7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_total_file_size(hdfs_url, hdfs_directory):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    total_size = 0\n",
    "\n",
    "    def get_directory_size(directory):\n",
    "        nonlocal total_size\n",
    "        contents = client.list(directory)\n",
    "        for item in contents:\n",
    "            item_path = f\"{directory}/{item}\"\n",
    "            if client.status(item_path)['type'] == 'DIRECTORY':\n",
    "                get_directory_size(item_path)\n",
    "            else:\n",
    "                total_size += client.status(item_path)['length']\n",
    "\n",
    "    get_directory_size(hdfs_directory)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "hdfs_url = 'hdfs://localhost:9000'\n",
    "hdfs_directory = '/user/data'\n",
    "\n",
    "total_size = calculate_total_file_size(hdfs_url, hdfs_directory)\n",
    "print(f\"Total file size in HDFS directory '{hdfs_directory}': {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4d2c3",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(TopNWords, self).configure_args()\n",
    "        self.add_passthru_arg('--top-n', type=int, help='Number of top words to display')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer_find_top_n)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.top_n = self.options.top_n\n",
    "        self.heap = []\n",
    "\n",
    "    def reducer_find_top_n(self, _, count_word_pairs):\n",
    "        for count, word in count_word_pairs:\n",
    "            heapq.heappush(self.heap, (count, word))\n",
    "            if len(self.heap) > self.top_n:\n",
    "                heapq.heappop(self.heap)\n",
    "\n",
    "        while self.heap:\n",
    "            count, word = heapq.heappop(self.heap)\n",
    "            yield word, count\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for word, count in self.heap:\n",
    "            yield word, count\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d307ea",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_namenode_health(namenode_url):\n",
    "    url = f\"{namenode_url}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['beans'][0]['State'] == 'active':\n",
    "        print(\"NameNode is active and healthy.\")\n",
    "    else:\n",
    "        print(\"NameNode is not active or healthy.\")\n",
    "\n",
    "def check_datanode_health(datanode_url):\n",
    "    url = f\"{datanode_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-0\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['beans'][0]['FSDatasetState'] == 'NORMAL':\n",
    "        print(\"DataNode is active and healthy.\")\n",
    "    else:\n",
    "        print(\"DataNode is not active or healthy.\")\n",
    "\n",
    "\n",
    "namenode_url = 'http://namenode:50070'\n",
    "check_namenode_health(namenode_url)\n",
    "\n",
    "\n",
    "datanode_url = 'http://datanode:50075'\n",
    "check_datanode_health(datanode_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e955c",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b932f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    client = InsecureClient(hdfs_url)\n",
    "    contents = client.list(hdfs_path, status=True)\n",
    "\n",
    "    print(f\"Listing contents of HDFS path: {hdfs_path}\")\n",
    "    for item in contents:\n",
    "        path = item['path']\n",
    "        if item['type'] == 'DIRECTORY':\n",
    "            print(f\"[DIR] {path}\")\n",
    "        else:\n",
    "            print(f\"[FILE] {path}\")\n",
    "\n",
    "hdfs_url = 'hdfs://localhost:9000'\n",
    "hdfs_path = '/user/data'\n",
    "\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ffd14",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62889b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_storage_utilization(hdfs_url):\n",
    "    url = f\"{hdfs_url}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    datanodes = data['beans']\n",
    "    storage_utilization = []\n",
    "\n",
    "    for datanode in datanodes:\n",
    "        node_name = datanode['name'].replace('DataNodeInfo', '')\n",
    "        capacity = datanode['Capacity']\n",
    "        remaining = datanode['Remaining']\n",
    "\n",
    "        storage_utilization.append((node_name, capacity, remaining))\n",
    "\n",
    "    storage_utilization.sort(key=lambda x: float(x[1]), reverse=True)\n",
    "\n",
    "    print(\"Storage Utilization Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Node\\t\\tCapacity\\tRemaining\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for node in storage_utilization:\n",
    "        print(f\"{node[0]}\\t{node[1]}\\t{node[2]}\")\n",
    "\n",
    "    highest_capacity_node = storage_utilization[0]\n",
    "    lowest_capacity_node = storage_utilization[-1]\n",
    "\n",
    "    print(\"\\nNode with Highest Capacity:\")\n",
    "    print(f\"Node: {highest_capacity_node[0]}\")\n",
    "    print(f\"Capacity: {highest_capacity_node[1]}\")\n",
    "    print(f\"Remaining: {highest_capacity_node[2]}\")\n",
    "\n",
    "    print(\"\\nNode with Lowest Capacity:\")\n",
    "    print(f\"Node: {lowest_capacity_node[0]}\")\n",
    "    print(f\"Capacity: {lowest_capacity_node[1]}\")\n",
    "    print(f\"Remaining: {lowest_capacity_node[2]}\")\n",
    "\n",
    "# Replace 'http://datanode:50075' with the URL of your DataNode\n",
    "hdfs_url = 'http://datanode:50075'\n",
    "\n",
    "analyze_storage_utilization(hdfs_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63d69c",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, job_file_path):\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'application-id' in data['application']:\n",
    "        application_id = data['application']['application-id']\n",
    "        print(f\"Submitted Hadoop job. Application ID: {application_id}\")\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "        return\n",
    "\n",
    "    submit_job_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}/submit\"\n",
    "    files = {'file': open(job_file_path, 'rb')}\n",
    "    response = requests.post(submit_job_url, files=files)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'application-id' in data['app']:\n",
    "        print(\"Hadoop job submitted successfully.\")\n",
    "        return application_id\n",
    "    else:\n",
    "        print(\"Failed to submit Hadoop job.\")\n",
    "        return\n",
    "\n",
    "def monitor_job_progress(resource_manager_url, application_id):\n",
    "    status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{application_id}\"\n",
    "    while True:\n",
    "        response = requests.get(status_url)\n",
    "        data = response.json()\n",
    "\n",
    "        if 'app' in data and 'state' in data['app']:\n",
    "            state = data['app']['state']\n",
    "            if state == 'FINISHED':\n",
    "                print(\"Hadoop job finished.\")\n",
    "                return\n",
    "            elif state == 'FAILED':\n",
    "                print(\"Hadoop job failed.\")\n",
    "                return\n",
    "            elif state == 'KILLED':\n",
    "                print(\"Hadoop job killed.\")\n",
    "                return\n",
    "            else:\n",
    "                progress = data['app']['progress']\n",
    "                print(f\"Job progress: {progress}%\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve job status.\")\n",
    "            return\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(resource_manager_url, application_id):\n",
    "    log_url = f\"{resource_manager_url}/proxy/{application_id}/logs/\"\n",
    "    response = requests.get(log_url)\n",
    "    print(\"Job output:\")\n",
    "    print(response.text)\n",
    "\n",
    "# Replace 'http://resource_manager:8088' with the URL of your YARN ResourceManager\n",
    "resource_manager_url = 'http://resource_manager:8088'\n",
    "# Replace 'path/to/your/job.jar' with the actual path to your Hadoop job JAR file\n",
    "job_file_path = 'path/to/your/job.jar'\n",
    "\n",
    "application_id = submit_hadoop_job(resource_manager_url, job_file_path)\n",
    "if application_id:\n",
    "    monitor_job_progress(resource_manager_url, application_id)\n",
    "    retrieve_job_output(resource_manager_url, application_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef825d7",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class WordCountJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(WordCountJob, self).configure_args()\n",
    "        self.add_passthru_arg('--input-split-size', type=int, default=64,\n",
    "                              help='Input split size in MB')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.strip().split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    split_sizes = [32, 64, 128, 256]  # Different input split sizes in MB\n",
    "\n",
    "    for split_size in split_sizes:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Replace '<input_file>' with the actual path to your input file\n",
    "        input_file = '<input_file>'\n",
    "        output_dir = f'output_{split_size}'\n",
    "\n",
    "        job = WordCountJob(args=[input_file, f'--output-dir={output_dir}',\n",
    "                                 f'--input-split-size={split_size}'])\n",
    "        with job.make_runner() as runner:\n",
    "            runner.run()\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"Input Split Size: {split_size} MB | Execution Time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f3269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
